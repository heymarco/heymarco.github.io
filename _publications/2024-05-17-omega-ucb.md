---
title: "Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals"
authors: "Marco Heyden, Vadim Arzamasov, Edouard Fouché, Klemens Böhm"
collection: publications
permalink: /publication/2024-05-17-omega-ucb
excerpt: 'We present a UCB-sampling policy for the Budgeted MAB problem that uses asymmetric confidence intervals to overcome issues of existing policies; our policy achieves logarithmic regret and outperforms existing policies in synthetic and real settings.'
date: 2024-05-17
venue: "KDD '24"
---
We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from $K$ arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current approaches for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, $\omega$-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has sublinear instance-dependent regret in general and logarithmic regret for parameter $\rho\geq 1$, and that it outperforms existing policies consistently in synthetic and real settings.

<a class="btn" style="text-decoration: none;" href="/files/Budgeted_Bandits_with_Asymmetric_Confidence_Intervals__KDD_24.pdf" rel="permalink">Access paper</a> or have a look at our [poster](/files/omega_ucb_poster.pdf).

You can also watch our animated [teaser video on youtube](https://youtu.be/tRQ1-DiXIME?si=2fxHZoI5E6e_xveu).
